{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport numpy as np\\nimport pandas as pd\\nfrom tqdm import tqdm\\ntqdm.pandas()\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\\nfrom sklearn import metrics\\nimport tensorflow as tf\\nimport gensim\\n\\nfrom tensorflow.python.keras.models import Model\\nfrom tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, Layer, SpatialDropout1D, Bidirectional, CuDNNGRU\\nfrom tensorflow.python.keras.optimizers import RMSprop\\nfrom tensorflow.python.keras.initializers import *\\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.python.keras import backend as K\\n\\nimport os\\nprint(os.listdir('../input'))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, Layer, SpatialDropout1D, Bidirectional, CuDNNGRU\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.initializers import *\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "import os\n",
    "print(os.listdir('../input'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import *\n",
    "import keras.backend as K\n",
    "from keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.6-tf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "print(\"Train shape : \", train.shape)\n",
    "print(\"Test shape : \", test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].str.lower()\n",
    "test[\"question_text\"] = test[\"question_text\"].str.lower()\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = None # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 72 # max number of words in a question to use #99.99%\n",
    "\n",
    "## fill up the missing values\n",
    "X = train[\"question_text\"].fillna(\"_na_\").values\n",
    "X_test = test[\"question_text\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "tokenizer.fit_on_texts(list(X))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "## Pad the sentences \n",
    "X = pad_sequences(X, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "Y = train['target'].values\n",
    "\n",
    "sub = test[['qid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "max_features = len(word_index)+1\n",
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n",
    "    \n",
    "    # making word distribution for 'oov'\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100 and o.split(\" \")[0] in word_index )\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100 and o.split(\" \")[0] in word_index)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    \n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#emb = gensim.models.KeyedVectors.load_word2vec_format('../input/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196219, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_1 = load_glove(word_index)\n",
    "#embedding_matrix_2 = load_fasttext(word_index)\n",
    "embedding_matrix_3 = load_para(word_index)\n",
    "embedding_matrix = np.mean((embedding_matrix_1, embedding_matrix_3), axis=0)  \n",
    "del embedding_matrix_1, embedding_matrix_3\n",
    "\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capsule():\n",
    "    K.clear_session()       \n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(rate=0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(100, return_sequences=True, \n",
    "                                kernel_initializer=glorot_normal(seed=12300), recurrent_initializer=orthogonal(gain=1.0, seed=10000)))(x)\n",
    "    x = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(100, activation=\"relu\", kernel_initializer=glorot_normal(seed=12300))(x)\n",
    "    x = Dropout(0.12)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(),)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_smart(y_true, y_pred):\n",
    "    args = np.argsort(y_pred)\n",
    "    tp = y_true.sum()\n",
    "    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n",
    "    res_idx = np.argmax(fs)\n",
    "    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 72, 300)           58865700  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 72, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 72, 200)           241200    \n",
      "_________________________________________________________________\n",
      "capsule_1 (Capsule)          (None, 10, 10)            20000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 59,137,501\n",
      "Trainable params: 271,601\n",
      "Non-trainable params: 58,865,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/6\n",
      " - 80s - loss: 0.1382 - val_loss: 0.1041\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10410, saving model to weights_best.h5\n",
      "Epoch 2/6\n",
      " - 78s - loss: 0.1078 - val_loss: 0.1020\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10410 to 0.10196, saving model to weights_best.h5\n",
      "Epoch 3/6\n",
      " - 79s - loss: 0.1017 - val_loss: 0.1020\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.10196\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "Epoch 4/6\n",
      " - 79s - loss: 0.0963 - val_loss: 0.1017\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10196 to 0.10171, saving model to weights_best.h5\n",
      "Epoch 5/6\n",
      " - 79s - loss: 0.0931 - val_loss: 0.0962\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10171 to 0.09621, saving model to weights_best.h5\n",
      "Epoch 6/6\n",
      " - 79s - loss: 0.0903 - val_loss: 0.0975\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.09621\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "Optimal F1: 0.6921 at threshold: 0.2688\n",
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/6\n",
      " - 80s - loss: 0.1368 - val_loss: 0.1108\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11083, saving model to weights_best.h5\n",
      "Epoch 2/6\n",
      " - 79s - loss: 0.1066 - val_loss: 0.1086\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11083 to 0.10856, saving model to weights_best.h5\n",
      "Epoch 3/6\n",
      " - 79s - loss: 0.1006 - val_loss: 0.1003\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10856 to 0.10026, saving model to weights_best.h5\n",
      "Epoch 4/6\n",
      " - 79s - loss: 0.0964 - val_loss: 0.0992\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10026 to 0.09923, saving model to weights_best.h5\n",
      "Epoch 5/6\n",
      " - 79s - loss: 0.0931 - val_loss: 0.0997\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.09923\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "Epoch 6/6\n",
      " - 79s - loss: 0.0875 - val_loss: 0.0970\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09923 to 0.09703, saving model to weights_best.h5\n",
      "Optimal F1: 0.6924 at threshold: 0.3603\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Epoch 1/6\n",
      " - 79s - loss: 0.1375 - val_loss: 0.1058\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10580, saving model to weights_best.h5\n",
      "Epoch 2/6\n",
      " - 79s - loss: 0.1068 - val_loss: 0.1019\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10580 to 0.10188, saving model to weights_best.h5\n",
      "Epoch 3/6\n",
      " - 79s - loss: 0.1012 - val_loss: 0.0992\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10188 to 0.09918, saving model to weights_best.h5\n",
      "Epoch 4/6\n",
      " - 79s - loss: 0.0973 - val_loss: 0.1003\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.09918\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "Epoch 5/6\n",
      " - 79s - loss: 0.0913 - val_loss: 0.0984\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09918 to 0.09845, saving model to weights_best.h5\n",
      "Epoch 6/6\n",
      " - 79s - loss: 0.0887 - val_loss: 0.0967\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09845 to 0.09675, saving model to weights_best.h5\n",
      "Optimal F1: 0.6900 at threshold: 0.3253\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Epoch 1/6\n",
      " - 80s - loss: 0.1377 - val_loss: 0.1071\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10709, saving model to weights_best.h5\n",
      "Epoch 2/6\n",
      " - 79s - loss: 0.1080 - val_loss: 0.1037\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10709 to 0.10367, saving model to weights_best.h5\n",
      "Epoch 3/6\n",
      " - 79s - loss: 0.1020 - val_loss: 0.1003\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10367 to 0.10033, saving model to weights_best.h5\n",
      "Epoch 4/6\n",
      " - 79s - loss: 0.0973 - val_loss: 0.0982\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10033 to 0.09818, saving model to weights_best.h5\n",
      "Epoch 5/6\n",
      " - 79s - loss: 0.0937 - val_loss: 0.0980\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09818 to 0.09796, saving model to weights_best.h5\n",
      "Epoch 6/6\n",
      " - 79s - loss: 0.0905 - val_loss: 0.0975\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09796 to 0.09749, saving model to weights_best.h5\n",
      "Optimal F1: 0.6834 at threshold: 0.3504\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Epoch 1/6\n",
      " - 80s - loss: 0.1374 - val_loss: 0.1092\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10917, saving model to weights_best.h5\n",
      "Epoch 2/6\n",
      " - 79s - loss: 0.1067 - val_loss: 0.1026\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10917 to 0.10264, saving model to weights_best.h5\n",
      "Epoch 3/6\n",
      " - 79s - loss: 0.1012 - val_loss: 0.0999\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10264 to 0.09992, saving model to weights_best.h5\n",
      "Epoch 4/6\n",
      " - 79s - loss: 0.0969 - val_loss: 0.1005\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.09992\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "Epoch 5/6\n",
      " - 79s - loss: 0.0912 - val_loss: 0.0979\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09992 to 0.09791, saving model to weights_best.h5\n",
      "Epoch 6/6\n",
      " - 79s - loss: 0.0884 - val_loss: 0.0991\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.09791\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "Optimal F1: 0.6880 at threshold: 0.3528\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=10, shuffle=True)\n",
    "bestscore = []\n",
    "y_test = np.zeros((X_test.shape[0], ))\n",
    "for i, (train_index, valid_index) in enumerate(kfold.split(X, Y)):\n",
    "    X_train, X_val, Y_train, Y_val = X[train_index], X[valid_index], Y[train_index], Y[valid_index]\n",
    "    filepath=\"weights_best.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=1, min_lr=0.0001, verbose=2)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=2, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr]\n",
    "    model = capsule()\n",
    "    if i == 0:print(model.summary()) \n",
    "    model.fit(X_train, Y_train, batch_size=512, epochs=6, validation_data=(X_val, Y_val), verbose=2, callbacks=callbacks)\n",
    "    model.load_weights(filepath)\n",
    "    y_pred = model.predict([X_val], batch_size=1024, verbose=2)\n",
    "    y_test += np.squeeze(model.predict([X_test], batch_size=1024, verbose=2))/5\n",
    "    f1, threshold = f1_smart(np.squeeze(Y_val), np.squeeze(y_pred))\n",
    "    print('Optimal F1: {:.4f} at threshold: {:.4f}'.format(f1, threshold))\n",
    "    bestscore.append(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape((-1, 1))\n",
    "pred_test_y = (y_test>np.mean(bestscore)).astype(int)\n",
    "sub['prediction'] = pred_test_y\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "5        0\n",
       "6        0\n",
       "7        0\n",
       "8        0\n",
       "9        0\n",
       "10       0\n",
       "11       0\n",
       "12       0\n",
       "13       0\n",
       "14       0\n",
       "15       0\n",
       "16       0\n",
       "17       0\n",
       "18       0\n",
       "19       0\n",
       "20       0\n",
       "21       0\n",
       "22       0\n",
       "23       0\n",
       "24       0\n",
       "25       0\n",
       "26       0\n",
       "27       0\n",
       "28       0\n",
       "29       0\n",
       "        ..\n",
       "56340    0\n",
       "56341    0\n",
       "56342    1\n",
       "56343    0\n",
       "56344    0\n",
       "56345    1\n",
       "56346    0\n",
       "56347    0\n",
       "56348    0\n",
       "56349    0\n",
       "56350    0\n",
       "56351    0\n",
       "56352    0\n",
       "56353    0\n",
       "56354    0\n",
       "56355    0\n",
       "56356    0\n",
       "56357    0\n",
       "56358    0\n",
       "56359    0\n",
       "56360    0\n",
       "56361    0\n",
       "56362    0\n",
       "56363    0\n",
       "56364    0\n",
       "56365    0\n",
       "56366    0\n",
       "56367    0\n",
       "56368    0\n",
       "56369    1\n",
       "Name: prediction, Length: 56370, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub['prediction'] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
